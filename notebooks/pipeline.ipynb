{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ab2c5026",
   "metadata": {},
   "source": [
    "# Ricola Robust Journey Planner\n",
    "This notebook contains the data pipeline and the implementation of the Journey Planner of the group *Ricola*. \n",
    "\n",
    "We implement a *robust* journey planner for the Zurich area. Given a desidered arrival time, the route planner computes the fastet route between departure and arrival stations and the probability of success of journey. You can find a detailed explaination of our solution in each section. \n",
    "\n",
    "\n",
    "\n",
    "**Table of contents**\n",
    "\n",
    "0. Set up environment\n",
    "1. Data Wrangling\n",
    "    - 1a. Extract Zurich stops\n",
    "    - 1b. Extract Zurich delays\n",
    "    - 1c. Timetables\n",
    "    - 1d. Walking paths\n",
    "    - 1e. Storing the data\n",
    "2. Delay prediction\n",
    "3. CSA: journey planning\n",
    "4. Data loading\n",
    "5. Validation\n",
    "6. User interface\n",
    "\n",
    "Note that if section 1 has been run already and the precomputed data has been successfully stored, it is possible to run only sections 0 and 2 to 6. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6894e2bd-9a56-45e6-9f17-d5b6c6a13989",
   "metadata": {},
   "source": [
    "## 0. Set up environment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cef5d15-22c9-4752-8576-fb0b1f301847",
   "metadata": {},
   "source": [
    "Here we connect the notebook with the Spark cluster, on which we will carry out all the distributed computations. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdb6b718-23a6-4f44-a37b-260fcdfc8b41",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext sparkmagic.magics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e62a41d-d5ad-4921-af63-6516f93b4b5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from IPython import get_ipython\n",
    "username = os.environ['RENKU_USERNAME']\n",
    "server = \"http://iccluster029.iccluster.epfl.ch:8998\"\n",
    "\n",
    "# set the application name as \"<your_gaspar_id>-homework3\"\n",
    "get_ipython().run_cell_magic(\n",
    "    'spark',\n",
    "    line='config', \n",
    "    cell=\"\"\"{{ \"name\": \"{0}-finalproject\", \"executorMemory\": \"4G\", \"executorCores\": 8, \"numExecutors\": 10, \"driverMemory\": \"8G\"}}\"\"\".format(username)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77d9e60c-3739-47be-92ae-11c225459d7e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "get_ipython().run_line_magic(\n",
    "    \"spark\", \"add -s {0}-finalproject -l python -u {1} -k\".format(username, server)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31e28819-300a-4ba6-a661-0f24079f79e7",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26406e3b-5782-4aa2-89b0-f2e149602522",
   "metadata": {},
   "source": [
    "## 1. Data Wrangling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e1a779c-ee22-4042-af4e-ba466aa0e3b1",
   "metadata": {},
   "source": [
    "In this section we prepare the necessary data on which our application will operate. \n",
    "We will use the following datasets: \n",
    "- SBB Istdaten (to compute the probability of success of a journey)\n",
    "- SBB Allstops (to extract the stops in the Zurich area)\n",
    "- SBB GTFS Timetables (to compute the possible journeys) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "216ab698-5c27-48bf-a0cc-8f2517dd27d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%spark\n",
    "import pandas as pd\n",
    "import pyspark.sql.functions as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cedfa45f-518a-4723-ae46-308f69f42aae",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%spark\n",
    "sbb_istdaten = spark.read.orc('/data/sbb/part_orc/istdaten/')\n",
    "allstops = spark.read.orc('/data/sbb/orc/allstops')\n",
    "sbb_istdaten.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f53dcd8c-b6e4-4a2b-9f48-5c698b5b6b47",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%spark\n",
    "sbb_istdaten.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "323958d2-cb39-4ce8-b30c-cb55cdac0a51",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%spark\n",
    "sbb_istdaten.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce51d576-681e-4345-b40b-a8eb533e2277",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%spark\n",
    "from functools import reduce\n",
    "\n",
    "oldColumns = sbb_istdaten.schema.names\n",
    "newColumns = [\"date\", \"trip_id\", \"operator_id\", \"operator_short\", \"operator_name\", \"ttype\", \"train_number\",\n",
    "              \"train_type\", \"umlauf_id\", \"train_service\", \"not_in_regular_schedule\", \"trip_failed\", \"bpuic\",\n",
    "              \"stop_name\", \"arrival_time\", \"actual_arrival\", \"arrival_measure\", \"departure_time\", \"actual_departure\",\n",
    "              \"departure_measure\", \"transport_continues\", \"year\", \"month\"]\n",
    "\n",
    "sbb_istdaten = reduce(lambda sbb_istdaten, idx: sbb_istdaten.withColumnRenamed(oldColumns[idx], newColumns[idx]), xrange(len(oldColumns)), sbb_istdaten)\n",
    "sbb_istdaten.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56c88ef9-9b8c-4a47-bbd7-90c3d3ef50c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%spark\n",
    "allstops.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "836f57a5-a588-459a-a0d2-ffa6cf0c6daa",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8b8fb9a-64a8-48aa-9a00-7b212b92ba35",
   "metadata": {},
   "source": [
    "### 1a. Extract Zurich Stops"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae987200-8684-440c-bb55-d57dceb93a73",
   "metadata": {},
   "source": [
    "We start by selecting only the stations which are within 20km of the center of Zurich. Even though we assume that the user will depart from and arrive to a station lcoated within a radius of 10km from Zurich, we keep all the stations within a radius of 20km since certains journeys can include intermediate stops outside of the 10km radius."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab20e738-73b3-42cd-b09e-3979d6a851c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%spark\n",
    "ZURICH_LAT = 47.378177\n",
    "ZURICH_LON = 8.540192\n",
    "MAX_DISTANCE = 20 # kms\n",
    "from math import sin, cos, sqrt, atan2, radians\n",
    "\n",
    "@F.udf\n",
    "def distance(lat1, lon1, lat2=ZURICH_LAT, lon2=ZURICH_LON): \n",
    "    # approximate radius of earth in km\n",
    "    R = 6373.0\n",
    "\n",
    "    lat1 = radians(lat1)\n",
    "    lon1 = radians(lon1)\n",
    "    lat2 = radians(lat2)\n",
    "    lon2 = radians(lon2)\n",
    "\n",
    "    dlon = lon2 - lon1\n",
    "    dlat = lat2 - lat1\n",
    "\n",
    "    a = sin(dlat / 2)**2 + cos(lat1) * cos(lat2) * sin(dlon / 2)**2\n",
    "    c = 2 * atan2(sqrt(a), sqrt(1 - a))\n",
    "\n",
    "    return R * c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cea8dacc-835e-46cf-bd18-3fe8eaa0c9ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%spark\n",
    "zurich_stops = allstops.withColumn(\"distance_from_zurich\", distance(allstops[\"stop_lat\"],allstops[\"stop_lon\"]))\\\n",
    "                                    .filter(F.col('distance_from_zurich')<=MAX_DISTANCE).cache()\n",
    "zurich_stops.show(5)\n",
    "print('We obtained {0} stops'.format(zurich_stops.count()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "052da157-c54e-405e-b815-d0ce60762137",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1608fee-1a6e-4ffb-b448-5a0557173fe4",
   "metadata": {},
   "source": [
    "### 1b. Extract delays in Zurich"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f504b96-0747-4d3a-be72-2e7ddf7cecaf",
   "metadata": {},
   "source": [
    "Now that we filtered only stops in Zurich, we need to filter SBB `istdaten` in order to remove all the means of transport that don't go through one of those stops. The `istdaten` dataset only contains stop names. For this reason we will perform a JOIN operation on the distinct stop names obtained from the `zurich_stops` table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a625590-7c8f-4c7e-bb95-22ef4c80896e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%spark\n",
    "distinct_zurich_stop_names = zurich_stops.select('stop_name').distinct()\n",
    "zurich_istdaten = sbb_istdaten.join(distinct_zurich_stop_names, 'stop_name', 'inner')                  \n",
    "zurich_istdaten.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dca04696-9440-4e1b-9f93-5f480f2d14d2",
   "metadata": {},
   "source": [
    "We further remove irregular connections from the `zurich_istdaten` dataset. Besides trips that failed or were not scheduled, we remove those observations for which the arrival information is either imprecise or missing. This is done because the historical arrival dealys will be key in computing the confidence interval of a journey. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a08b139b-4057-4045-9445-3ea60041a80d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%spark\n",
    "# Filter out observations that do not have arrival time or had AN_PROGNOSE_STATUS differne to REAL or GESCHAETZT\n",
    "zurich_istdaten_filtered = zurich_istdaten.filter((zurich_istdaten.arrival_measure == 'REAL') | (zurich_istdaten.arrival_measure == 'GESCHAETZT'))\\\n",
    "                                          .filter((zurich_istdaten.arrival_time.isNotNull()) & (zurich_istdaten.arrival_time != ''))\\\n",
    "                                          .filter(zurich_istdaten.not_in_regular_schedule == False)\\\n",
    "                                          .filter(zurich_istdaten.trip_failed == False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f6dbda2-e9c0-497f-8c08-cf4b998993e1",
   "metadata": {},
   "source": [
    "Now for each connection we compute the arrival delay, the day of the week when it happened and the hour of the day.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d9bf027-b477-4234-8aa0-1ee4bd59ed52",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%spark\n",
    "import pyspark.sql.functions as F\n",
    "zurich_istdaten_delays = zurich_istdaten_filtered.withColumn('delay_arrival', F.round((F.unix_timestamp('actual_arrival', \"dd.MM.yyyy HH:mm:ss\") - F.unix_timestamp('arrival_time', \"dd.MM.yyyy HH:mm\"))/60))\\\n",
    "                                                 .withColumn('day_of_week', F.dayofweek(F.from_unixtime(F.unix_timestamp(zurich_istdaten_filtered.date, \"dd.MM.yyyy\"), \"yyyy-MM-dd\")))\\\n",
    "                                                 .withColumn('hour', F.hour(F.from_unixtime(F.unix_timestamp(zurich_istdaten_filtered.arrival_time, \"dd.MM.yyyy HH:mm\"), \"yyyy-MM-dd HH:mm\")))\\\n",
    "                                                 .drop(\"operator_id\", \"operator_short\", \"operator_name\", \"umlauf_id\",\n",
    "                                                       \"train_service\", \"not_in_regular_schedule\", \"trip_failed\", \"bpuic\",\n",
    "                                                       \"arrival_measure\",  \"transport_continues\", \"date\", \"trip_id\", \"train_number\", \"arrival_time\",\n",
    "                                                      \"actual_arrival\", \"departure_time\", \"actial_departure\", \"departure_measure\", \"year\", \"month\").cache()\n",
    "zurich_istdaten_delays.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b67b327-104f-4516-a537-6465bb670a3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%spark\n",
    "zurich_istdaten_delays.groupBy('ttype').count().show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35a3b80a-dd69-4622-bac9-920efba17ef1",
   "metadata": {},
   "source": [
    "We notice that there are two separate entries both representing buses. We thus transform BUS into Bus to have all the transportation types in the same format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cce3a8f-b78d-47bb-be7d-3d5b81396e38",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%spark\n",
    "zurich_istdaten_delays = zurich_istdaten_delays.withColumn('ttype', F.when(zurich_istdaten_delays.ttype == \"BUS\",\"Bus\").otherwise(zurich_istdaten_delays.ttype))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01b6ee22-9b66-481c-96e7-6769c6ac3fa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%spark\n",
    "zurich_istdaten_delays.groupBy('ttype').count().show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "956f20e5-81c8-4bf9-a29d-5a1e6178275e",
   "metadata": {},
   "source": [
    "Furthermore, we notice that there are 27709 observations where the type of transportation is not specified. Since in the delay-prediction model we will be using historical data regarding the transportation line, we check if for these 22619 observations the `train_type` information are available. If this is not the case, we will remove these observations as they cannot be safely attributed to any transportation type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6f13bc2-c781-4c2f-a877-4957bdd4cc39",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%spark\n",
    "zurich_istdaten_delays.filter((zurich_istdaten_delays.ttype == '') | (zurich_istdaten_delays.ttype.isNull())).groupBy('train_type').count().show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "079c076e-ad54-4967-82b9-6f522c4f6caf",
   "metadata": {},
   "source": [
    "It seems that all these observation for which the type of transportation is not well defined, the transportation line is well defined. We double check this conclusion in the following cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44ec8e54-a093-41f9-9f74-6880639e56da",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%spark\n",
    "zurich_istdaten_delays.filter((zurich_istdaten_delays.ttype == '') | (zurich_istdaten_delays.ttype.isNull()))\\\n",
    "                      .filter((zurich_istdaten_delays.train_type == '') | (zurich_istdaten_delays.train_type.isNull()))\\\n",
    "                      .count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a1b4a2f-5063-4ff3-9372-6e9fcce1e4e9",
   "metadata": {},
   "source": [
    "Thus, no further cleaning is needed in the type of transportation column. Now we explore more in details the `train_type` column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "355450f1-24a7-497e-9ee9-fe0fa9fd4c51",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%spark\n",
    "zurich_istdaten_delays.filter((zurich_istdaten_delays.train_type == '') | (zurich_istdaten_delays.train_type.isNull())).count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e19a56f-525a-4dce-bfb0-59f1b9046eb0",
   "metadata": {},
   "source": [
    "Each observation in the dataset is assigned to a line. We now analyze the information available with respect to the day and hour. Firstly, we check if there are `null` values and we have a look at unique values for each column. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e1ea0df-ea09-4b43-afb7-0e28766adb41",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%spark\n",
    "zurich_istdaten_delays.groupBy('day_of_week').count().show(7)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b276caf2-315b-4c74-8532-4762f11aaaa3",
   "metadata": {},
   "source": [
    "Note: 1-Sunday, 2-Monday, 3-Tuesday, 4-Wednesday, 5-Thursday, 6-Friday, 7-Saturday\n",
    "\n",
    "As expected, Sunday and Saturday display the lowest number of observations. As simplyfying assumption, we build the journey planner only for days of the week. Let's filter out Saturday's and and Sunday's observations "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1452b188-7655-44d7-8765-c0aade4fc5c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%spark\n",
    "zurich_istdaten_delays = zurich_istdaten_delays.filter((F.col(\"day_of_week\") != 1)  & (F.col(\"day_of_week\") != 7)).cache()\n",
    "zurich_istdaten_delays.groupBy('day_of_week').count().show(7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04e3533c-30ab-47a6-bb7a-c86f56df9a3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%spark\n",
    "zurich_istdaten_delays.groupBy('hour').count().orderBy('hour').show(24)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00306536-f27b-482a-a11c-05379f366ad4",
   "metadata": {},
   "source": [
    "All the observations have the hour properly assigned. As expected, we notice that the number of observations between midnight and 5am is significantly smaller since most of the public transport does not run during the night.\n",
    "\n",
    "Finally, we check the information regarding the arrival delays to see the distribution. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f44baa5-3e82-4dbe-b5f3-73a9d5e42eee",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%spark\n",
    "zurich_istdaten_delays.groupBy('delay_arrival').count().orderBy('delay_arrival').show(5)\n",
    "# the user can change the number of observations visaulized to display the full distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "545b87bd-be60-46c2-a912-1d900c7dac62",
   "metadata": {},
   "source": [
    "It seems that there are many observations where the delay is negative. Let's understand why this might be the case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb6e93e0-6010-463b-933b-e495ec187b8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%spark\n",
    "zurich_istdaten_delays.filter(zurich_istdaten_delays.delay_arrival < -0).count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d390c2c-460e-4b02-a343-d60e54e83967",
   "metadata": {},
   "source": [
    "A total of 1698880 observations are characterized by a negative arrival delay. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98e411c3-cd2c-4718-b859-41ac1b6ac413",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%spark\n",
    "zurich_istdaten_delays.filter(zurich_istdaten_delays.delay_arrival < -40).count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a665375-d03e-4da6-b3a8-1d86d2801e8a",
   "metadata": {},
   "source": [
    "Only 599 connections arrived 40 minutes (or more) early. Let's try to understand why these few observations happened and decide whether it is safe to discard them as outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a23f3b4-8394-4662-8191-3a4444ef0598",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%spark\n",
    "zurich_istdaten_delays.filter(zurich_istdaten_delays.delay_arrival < -500).groupBy('ttype').count().show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0212c39-194f-4b77-b331-bdb356302b0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%spark\n",
    "zurich_istdaten_delays.filter(zurich_istdaten_delays.delay_arrival < -500).groupBy('train_type').count().orderBy(F.col(\"count\").desc()).show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6d03171-e121-41d4-8e3c-1443259fd3e4",
   "metadata": {},
   "source": [
    "We noticed that all the very extreme values are bus rides and, specifically, the lines 231, 352, and 350. It thus seems reasonable to assume that these extreme values are probably due to changes in the schedule. \n",
    "To prevent affecting the estimation of the probability significantly with these absurd values, we decide to drop all the observations for which the arrival \"delay\" is smaller than -40 minutes. \n",
    "\n",
    "Similarly, some connections are characterised by very large delays. While this is more natural than very early arrivals (e.g. there could be an accident in the highway and then a bus can be stuck in traffic for multiple hours), we believe that observations displaying more than 11 hours of delay are most likely due to external factors. Thus we remove delays larger than 700 minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1acdcb6-1103-491a-9f3d-7e61f952cd7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%spark\n",
    "zurich_istdaten_delays = zurich_istdaten_delays.filter((F.col(\"delay_arrival\") > -40)  & (F.col(\"delay_arrival\") < 700)).cache()\n",
    "zurich_istdaten_delays.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23f5fce9-b764-43b9-a0f7-e938aa33a138",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%spark\n",
    "zurich_istdaten_delays.select(F.mean('delay_arrival')).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8176197b-8282-4c1d-bce7-6a45d7edfcee",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%spark\n",
    "zurich_istdaten_delays.groupBy('ttype').agg(F.mean('delay_arrival')).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fd89d31-e1fc-4a63-8212-adfcaa84deb3",
   "metadata": {},
   "source": [
    "As expected, busses seam to display the largest average delay, probably due to the stronger dependency with traffic and external factors out of the control of the service provider. \n",
    "\n",
    "As better explained in the `Delay Prediction` section, the probability of succes of a journey is computed by multiplying the probability of success of each connection, where connection is defined as the quadruple (day_of_week, hour, location, train_type). To do so, we already precompute the empirical comulative distribuion of the arrival delays for each quadruple. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48369d51-0cad-45d1-9e4e-26624f88292d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%spark\n",
    "zurich_istdaten_delays = zurich_istdaten_delays.groupBy(\"stop_name\", \"ttype\", \"day_of_week\", \"hour\", \"delay_arrival\", \"train_type\").count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f81a744-83b2-410f-8bf0-4ce42fb3d078",
   "metadata": {},
   "source": [
    "Finally, for future purposes, we encode the `stop_name` using the utf-8 encoding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a835f413-9ce2-4c53-ab02-41f242cc5c67",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%spark\n",
    "zurich_istdaten_delays = zurich_istdaten_delays.withColumn('stop_name', F.encode(F.col('stop_name'), \"utf-8\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "722402e0",
   "metadata": {
    "tags": []
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dcb07a1-ef4d-473a-ab9a-97b70bc3006d",
   "metadata": {},
   "source": [
    "### 1c. Timetables\n",
    "We craete 5 tables, one for each weekday, of the week between Monday, May 13th 2019 to Friday, May 17th 2019. The journey planner will provide trips using this timetable as reference. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3794c774-6b7f-4bcc-bbbb-f1250f78afe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%spark\n",
    "# Load stop times for the corresponding week\n",
    "stop_times = spark.read.csv('/data/sbb/csv/stop_times/2019/05/15/stop_times.txt', header=True, encoding='utf8')\\\n",
    "    .drop('pickup_type','drop_off_type')\\\n",
    "    .filter((F.col('departure_time') < '24') & (F.col('arrival_time') < '24'))\n",
    "# Only take data for the city of Zurich\n",
    "zurich_stop_times = stop_times.join(zurich_stops.select('stop_id', 'stop_name'), 'stop_id').drop('stop_id')\n",
    "\n",
    "# Duplicate the table to separate departures and arrivals\n",
    "departures = zurich_stop_times\\\n",
    "    .withColumn('connection_id', F.concat('trip_id',F.lit('_'),'stop_sequence'))\\\n",
    "    .withColumnRenamed('stop_name', 'departure_stop')\\\n",
    "    .drop('arrival_time', 'stop_sequence')\n",
    "arrivals = zurich_stop_times\\\n",
    "    .withColumn('connection_id', F.concat('trip_id',F.lit('_'),(F.col('stop_sequence')-1).cast('integer')))\\\n",
    "    .withColumnRenamed('stop_name', 'arrival_stop')\\\n",
    "    .drop('departure_time', 'stop_sequence', 'trip_id')\n",
    "\n",
    "# Join departures and arrivals to obtain rows of connections between stopsb\n",
    "connections = departures.join(arrivals, 'connection_id')\\\n",
    "    .drop('connection_id')\\\n",
    "    .cache()\n",
    "\n",
    "connections.show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a534678d-865b-4d22-b776-3374545b3155",
   "metadata": {},
   "source": [
    "The obtained table does not discriminate between days of the week, such information can be found in the `calendar.txt` dataset. \n",
    "To join our `connections` table to `calendar` we need to perform an intermediate join with `trips.txt`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d7d7e45-3bff-495c-9c4a-62e7bc03bbd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%spark\n",
    "trips = spark.read.csv('/data/sbb/csv/trips/2019/05/15/trips.txt', header=True, encoding='utf8').select('trip_id', 'service_id', 'route_id')\n",
    "connections_extended = connections.join(trips, 'trip_id')\n",
    "\n",
    "calendar = spark.read.csv('/data/sbb/csv/calendar/2019/05/15/calendar.txt', header=True, encoding='utf8').drop('start_date','end_date')\n",
    "connections_with_days = connections_extended.join(calendar, 'service_id')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "264db733-30de-4e7d-9d03-5030482e4f7d",
   "metadata": {},
   "source": [
    "We also need to include information about the means of transport. Let's retrieve the name and the type for each connection.\n",
    "\n",
    "We will adopt some simplifying assumptions in order to avoid irregularities in data. Let's only take the most frequent means of transport."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c72b5342-c147-40fa-b7ef-874e5b40665e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%spark\n",
    "routes = spark.read.csv('/data/sbb/csv/routes/2019/05/15/routes.txt', header=True, encoding='utf8').select('route_id', 'route_short_name', 'route_desc')\n",
    "connections_with_means_names = connections_with_days\\\n",
    "    .join(routes, 'route_id')\n",
    "\n",
    "connections_with_means_names.groupBy('route_desc')\\\n",
    "    .count()\\\n",
    "    .orderBy(F.col('count').desc())\\\n",
    "    .show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e1e1c60-486b-46f7-ad7c-a1ac47d467ae",
   "metadata": {},
   "source": [
    "The most frequent means of transport are Bus, Tram and two different type of trains. We will feed only these connections to our route-finding algorithm.\n",
    "\n",
    "We also remove fabricated rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "629b950b-5a6d-48c1-b207-2278aa97fc4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%spark\n",
    "connections_filtered = connections_with_means_names\\\n",
    "    .filter(F.col('route_desc').isin(['Bus', 'Tram', 'S-Bahn', 'InterRegio']))\\\n",
    "    .filter(~F.col('route_short_name').contains('Y'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05005eb6-b78b-4dac-80fe-f0f99a06181e",
   "metadata": {},
   "source": [
    "To allow interactions between `timetables` and `istdaten` later down the pipeline, we need to retrieve short names and types of the 4 selected means of transport."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5daa88bd-f59f-4961-a3ed-c0b3c0bd37d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%spark\n",
    "@F.udf\n",
    "def retrieve_transportation_name(route_desc, route_short_name):\n",
    "    def train_name(train_name, train_number):\n",
    "        long_to_short_name = {\n",
    "            'S-Bahn':'S',\n",
    "            'InterRegio':'IR'\n",
    "        }\n",
    "        return long_to_short_name[train_name]+str(train_number)\n",
    "    \n",
    "    return route_short_name if route_desc in ['Bus', 'Tram'] else train_name(route_desc, route_short_name)\n",
    "\n",
    "@F.udf\n",
    "def retrieve_transportation_type(route_desc, route_short_name):    \n",
    "    return route_desc if route_desc in ['Bus', 'Tram'] else 'Zug'\n",
    "\n",
    "weekly_timetable = connections_filtered\\\n",
    "    .withColumn('route_short_name',F.col('route_short_name').cast('int'))\\\n",
    "    .withColumn('transportation_name', retrieve_transportation_name(F.col('route_desc'), F.col('route_short_name')))\\\n",
    "    .withColumn('transportation_type', retrieve_transportation_type(F.col('route_desc'), F.col('route_short_name')))\\\n",
    "    .drop('route_id', 'service_id', 'route_desc', 'route_short_name')\n",
    "\n",
    "weekly_timetable.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77645de4-f261-4c3c-85ee-d697dfc4a9ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%spark\n",
    "weekly_timetable.select('transportation_name','transportation_type').distinct()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba249f99-a02d-4115-8899-61339077fb2e",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "589d3367-6da4-4f94-a454-2273570c406e",
   "metadata": {},
   "source": [
    "### 1d. Walking paths"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8660daa-0742-44e0-a436-c6d62f6dc037",
   "metadata": {},
   "source": [
    "We allow short (max 500m) walking distances for transfers between two stops, and assume a walking speed of 50m/1min on a straight line, regardless of obstacles, human-built or natural, such as building, highways, rivers, or lakes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd3c9318-4327-4366-a171-f7df3450c96f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%spark\n",
    "stops_relevant_info = zurich_stops.select('stop_name', 'stop_lat', 'stop_lon')\n",
    "walking_departure_stops = stops_relevant_info.alias('departure')\\\n",
    "    .withColumnRenamed('stop_name', 'dep_stop_name')\n",
    "walking_arrival_stops = stops_relevant_info.alias('arrival')\\\n",
    "    .withColumnRenamed('stop_name', 'arr_stop_name')\n",
    "all_walks = walking_departure_stops.crossJoin(walking_arrival_stops)\\\n",
    "    .withColumn('dep_arr', F.concat('dep_stop_name','arr_stop_name'))\\\n",
    "    .dropDuplicates(['dep_arr'])\\\n",
    "    .drop('dep_arr')\\\n",
    "    .filter(F.col('dep_stop_name') != F.col('arr_stop_name'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "094a647b-50b1-4eae-807d-a6975d86d059",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%spark\n",
    "short_walks = all_walks.withColumn('distance', distance('departure.stop_lat', 'departure.stop_lon', 'arrival.stop_lat', 'arrival.stop_lon'))\\\n",
    "    .filter('distance < 0.5')\\\n",
    "    .withColumn('duration', F.ceil(F.col('distance') / 0.05))\\\n",
    "    .select('dep_stop_name', 'arr_stop_name', 'duration')\\\n",
    "    .withColumnRenamed('dep_stop_name', 'departure')\\\n",
    "    .withColumnRenamed('arr_stop_name', 'arrival')\n",
    "short_walks.show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad45fc8f-a791-4a73-94a2-3ac8e80f233d",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e29776a6-a872-4c21-bb91-7026efcd23b3",
   "metadata": {},
   "source": [
    "### 1e. Store the data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bf9285e-71da-4079-8b6b-49a34587e466",
   "metadata": {},
   "source": [
    "We store all the data in our folder on the IC cluster. This way we can avoid re-computing everything for each query.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ab977c6-a52a-4ffa-b038-f082af78a0ca",
   "metadata": {},
   "source": [
    "##### Zurich stops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88d9d4a2-5914-468c-a1c4-08b10b7014a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%spark\n",
    "zurich_stops.write.mode('overwrite').option(\"header\",\"true\").format(\"csv\").save('/group/ricola/zurich_stops.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf62db27-22ce-4cae-ab0d-9c89fc7755c5",
   "metadata": {},
   "source": [
    "##### Zurich delays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2665a381-9bbc-4c20-8946-c77bc68f6475",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%spark\n",
    "zurich_istdaten_delays.write.mode('overwrite').option(\"header\",\"true\").partitionBy(\"day_of_week\",\"hour\").format(\"orc\").save('/group/ricola/zurich_istdaten_delays.orc')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efaad11e-f8af-487e-a5c5-35a547391b00",
   "metadata": {},
   "source": [
    "##### Timetable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15c73b3c-daca-4a39-b23a-8db48f64a624",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%spark \n",
    "import pickle\n",
    "def create_weekday_dataframe(weekday_string):\n",
    "    return (weekly_timetable\n",
    "            .filter(F.col(weekday_string) == '1')\n",
    "            .select('trip_id','departure_time','departure_stop','arrival_time','arrival_stop','transportation_name','transportation_type'))\n",
    "\n",
    "for day in ['monday', 'tuesday', 'wednesday', 'thursday', 'friday']:\n",
    "    create_weekday_dataframe(day).write.mode('overwrite').option(\"header\",\"true\").format(\"csv\").save('/group/ricola/'+day+'.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feb91edc-996b-45e2-b750-82bcebd97cec",
   "metadata": {},
   "source": [
    "##### Walking paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0f6d827-c118-4728-8209-a1b3fa15a947",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%spark\n",
    "short_walks.write.mode(\"overwrite\").option(\"header\",\"true\").format(\"csv\").save('/group/ricola/short_walks.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7e0ce6a-e476-460f-b8ff-4d37c25f7e3c",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "118b08df-4f08-4c67-bd13-239991966c28",
   "metadata": {},
   "source": [
    "## 2. Delay prediction\n",
    "\n",
    "Now that the historical data have been properly processed, we can develop our predictive model. \n",
    "\n",
    "Given a specific journey plan, we estimate the probability that the user will arrive at destination by multiplying the probability of arriving on time to catch each connection and at the destination. That is:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bf706d4-b4f8-4b9d-952b-7cad4e85deb7",
   "metadata": {},
   "source": [
    "$P(\\text{journey is successful}) = \\prod_{j=1}^{C} P(\\text{connection j is successful}) \\\\\n",
    "\\qquad \\qquad \\qquad \\qquad \\! \\; \\; \\,= \\prod_{j=1}^{C} P(\\text{arrival } c_{j} \\leq \\text{time available until next departure})$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a88f038-2aaf-4160-8a7c-75fa83841327",
   "metadata": {},
   "source": [
    "To do so, we will group the historical data according to the following criteria:\n",
    "- `stop_name`: the bus/train/metro station where the user is taking the means of transport\n",
    "- `day_of_week`: which day the user is travelling\n",
    "- `hour`: at which hour of the day the next public transport is scheduled\n",
    "- `train_type`: the line on which the user is travelling to arrive at the station being studied\n",
    "\n",
    "Then, we can calculate the probability of succesfully catching the next connection as the percentage of times that the arriving connection arrived within `time_buffer` minutes of delay, where `time_buffer` is the difference between the scheduled arrival time and the next departure time, including the required time to change which we assume to be 1 minute. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78075405-d62d-41af-a7b5-2f47928f2501",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%spark\n",
    "import pandas as pd\n",
    "import pyspark.sql.functions as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a16a396-0361-408d-8be1-086c07a85e1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%spark\n",
    "def compute_probability(stop_name, day_of_week, train_type, hour, ttype, time_buffer):\n",
    "    total = zurich_istdaten_delays.filter((zurich_istdaten_delays.stop_name == stop_name) & \n",
    "                                                    (zurich_istdaten_delays.day_of_week == day_of_week) &\n",
    "                                                    (zurich_istdaten_delays.train_type == train_type) &\n",
    "                                                    (zurich_istdaten_delays.hour == hour) &\n",
    "                                                    (zurich_istdaten_delays.ttype == ttype)).select(F.sum('count')).collect()[0][0]\n",
    "    # If not enough information available\n",
    "    if total <= 10:\n",
    "        return 1.0\n",
    "    filtered = zurich_istdaten_delays.filter((zurich_istdaten_delays.stop_name == stop_name) & \n",
    "                                                    (zurich_istdaten_delays.day_of_week == day_of_week) &\n",
    "                                                    (zurich_istdaten_delays.train_type == train_type) &\n",
    "                                                    (zurich_istdaten_delays.hour == hour) &\n",
    "                                                    (zurich_istdaten_delays.delay_arrival <= time_buffer) &\n",
    "                                                    (zurich_istdaten_delays.ttype == ttype)).select(F.sum('count')).collect()[0][0]\n",
    "\n",
    "    confidence = float(filtered) / float(total)\n",
    "    \n",
    "    return confidence"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cf142ec-74b6-41e2-a337-4e39b19c2b82",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15ca2d82-5079-48a3-9b59-7e2dda5d153c",
   "metadata": {},
   "source": [
    "## 3. CSA: journey planning\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2ce584b-e298-47dc-baac-a103720ab504",
   "metadata": {},
   "source": [
    "To provide the best possible journey between two stop we implement a modified version of the *Connection Scan Algorithm*. \n",
    "\n",
    "Our algorithm computes the path backwards starting from the arrival stop to the departure one. The pseudocode of our implementation is shown below. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "826646f9-a444-4b7c-ae26-ef511850c78d",
   "metadata": {},
   "source": [
    "<img src=\"../notebooks/CSA-Ricola-implementation.jpg\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f96d62e-1ebc-4eba-b5fb-70e31b66c925",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%spark\n",
    "from datetime import time, datetime, date\n",
    "import pandas as pd\n",
    "def add_time(t, d):\n",
    "    return (datetime.combine(date(1,1,1), t) + d).time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53f671fc-9788-46f7-b9f1-7cb7f297f19d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%spark\n",
    "def find_path(S, starting_stop):\n",
    "    path = []\n",
    "    # step is the current connection\n",
    "    step = S[starting_stop]\n",
    "    old_trip_id = step[\"trip_id\"]\n",
    "    # to add is the route we're covering\n",
    "    to_add = step\n",
    "    # while we have not reached the end of the path\n",
    "    while step[\"trip_id\"]!= None:\n",
    "        # if the current trip_id is different than the previous one\n",
    "        if step[\"trip_id\"] != old_trip_id:\n",
    "            path.append(to_add)\n",
    "            to_add = step\n",
    "            old_trip_id = step[\"trip_id\"]\n",
    "        else:\n",
    "            to_add[\"arrival_time\"] = step[\"arrival_time\"]\n",
    "            to_add[\"arrival_stop\"] = step[\"arrival_stop\"]\n",
    "        step = S[step[\"arrival_stop\"]]\n",
    "    path.append(to_add)\n",
    "    return path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e05fbf36-3d58-4302-a136-b1818e4e8dd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%spark\n",
    "from collections import defaultdict\n",
    "\n",
    "delta = pd.to_timedelta(2, unit='m')\n",
    "\n",
    "def CSA(starting_stop, arrival_stop, arrival_time, connections, avoid=[]):\n",
    "    '''\n",
    "    Connections Scan Algorithm to maximize the starting time\n",
    "    \n",
    "    Return: List of trips, where each trip is a list of 5 elements: \n",
    "                [means of transport (trip ID or 'walking'), starting time (datetime), starting_stop (name), arriving_time (datetime), arriving stop (name)]\n",
    "                \n",
    "    '''   \n",
    "    \n",
    "    # Best possible connection\n",
    "    latest_departure_from = defaultdict(lambda: pd.Series(data = [None, time.min, None, time.max, None], index = ['trip_id', 'departure_time', 'departure_stop', 'arrival_time', 'arrival_stop']))\n",
    "    latest_departure_from[arrival_stop] = pd.Series(data = [None, arrival_time, None, time.max, None], index = ['trip_id', 'departure_time', 'departure_stop', 'arrival_time', 'arrival_stop'])\n",
    "    \n",
    "    # For all footpaths where footpath.arrival is arrival_stop\n",
    "    for i, footpath in footpaths[ footpaths[\"arrival\"] == arrival_stop ].iterrows():\n",
    "        # The latest departure is given by  #### arrival_time - footpath[\"duration\"]\n",
    "        latest_departure_from[footpath['departure']] = pd.Series(data = ['Walking', add_time(arrival_time, -footpath['duration']), footpath['departure'], arrival_time,arrival_stop, 'Walking', ''], index = ['trip_id', 'departure_time', 'departure_stop', 'arrival_time', 'arrival_stop', 'transportation_type', 'transportation_name'])\n",
    "       \n",
    "    # Take all connections via means of transport which arrive before wanted arrival time\n",
    "    connections = connections[connections['arrival_time']<arrival_time]\n",
    "    # We search among the connections whose arrival time is smaller than our arrival_time \n",
    "    for i, connection in connections.iterrows():\n",
    "        # If the connection is not to be avoided (due to previously examined paths)\n",
    "        if not (connection[\"trip_id\"] in avoid): \n",
    "            # If the latest departure time from the starting station is greater than the arrival\n",
    "            # time of the current connection, we have already found the optimal path\n",
    "            if latest_departure_from[starting_stop][\"departure_time\"] >= connection[\"arrival_time\"]:\n",
    "                break\n",
    "            #If the best trip for the next station is reachable in time \n",
    "            if latest_departure_from[connection[\"arrival_stop\"]][\"trip_id\"] == connection[\"trip_id\"] or (latest_departure_from[connection[\"arrival_stop\"]][\"departure_time\"] >= add_time(connection[\"arrival_time\"], delta)):\n",
    "                # Check if the connection that allows the latest departure time is the current one\n",
    "                if latest_departure_from[connection[\"departure_stop\"]][\"departure_time\"] < connection[\"departure_time\"]:\n",
    "                    latest_departure_from[connection[\"departure_stop\"]] = connection\n",
    "                    # We add footpath information for nearby stations\n",
    "                    for i, footpath in footpaths[ footpaths[\"arrival\"] == connection[\"departure_stop\"] ].iterrows():\n",
    "                        if latest_departure_from[footpath[\"departure\"]][\"departure_time\"] < (add_time(connection[\"departure_time\"], - footpath[\"duration\"])):\n",
    "                            latest_departure_from[footpath[\"departure\"]] = pd.Series(data = ['Walking', add_time(connection[\"departure_time\"],-footpath[\"duration\"]), footpath['departure'], connection[\"departure_time\"], footpath[\"arrival\"], 'Walking', ''], index = ['trip_id', 'departure_time', 'departure_stop', 'arrival_time', 'arrival_stop', 'transportation_type', 'transportation_name'])\n",
    "            \n",
    "    if latest_departure_from[starting_stop][\"trip_id\"] is not None:\n",
    "        return find_path(latest_departure_from, starting_stop)\n",
    "    else:\n",
    "        return []\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0a1e16e-69c4-4931-a9f9-65588dd2f802",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%spark\n",
    "week = {'Monday':2, 'Tuesday':3, 'Wednesday':4, 'Thursday':5, 'Friday':6}\n",
    "\n",
    "def difference(t1, t2):\n",
    "    return (datetime.combine(date(1,1,1), t1)-datetime.combine(date(1,1,1), t2)).seconds//60\n",
    "\n",
    "def confidence_interval(path, week_day, arrival_time):\n",
    "    p = 1.0\n",
    "    if len(path) < 1:\n",
    "        return 1\n",
    "    \n",
    "    for i,trip in enumerate(path): \n",
    "        if trip[\"trip_id\"] != \"Walking\":\n",
    "            # if it is not the last connection, we compare its arrival delay with the departure of the next connection minus the change time (1 minute)\n",
    "            if i < len(path)-1:\n",
    "                input_prob = (trip[\"arrival_stop\"], week[week_day], trip[\"transportation_name\"], trip[\"arrival_time\"].hour, trip[\"transportation_type\"], difference(path[i+1][\"departure_time\"], trip[\"arrival_time\"]) - 1)\n",
    "                p = p * compute_probability(input_prob[0],input_prob[1],input_prob[2],input_prob[3],input_prob[4],input_prob[5])\n",
    "            # if it is the last connection, then we compare its arrival delay with the desired arrival time of the user    \n",
    "            if i == len(path)-1:\n",
    "                input_prob = (trip[\"arrival_stop\"], week[week_day], trip[\"transportation_name\"], trip[\"arrival_time\"].hour, trip[\"transportation_type\"], difference(arrival_time, trip[\"arrival_time\"]))\n",
    "                p = p * compute_probability(input_prob[0],input_prob[1],input_prob[2],input_prob[3],input_prob[4],input_prob[5])                \n",
    "    return p\n",
    "\n",
    "def fix_walking(path):\n",
    "    if len(path)<2:\n",
    "        return path\n",
    "    for i,trip in enumerate(path): \n",
    "        if trip[\"trip_id\"] == \"Walking\" and i>0:\n",
    "            diff = difference(trip[\"departure_time\"], path[i-1][\"arrival_time\"])\n",
    "            trip[\"departure_time\"] = add_time(trip[\"departure_time\"], pd.to_timedelta(- diff, unit='m'))\n",
    "            trip[\"arrival_time\"] = add_time(trip[\"departure_time\"], pd.to_timedelta(- diff, unit='m'))\n",
    "    return path\n",
    "\n",
    "\n",
    "def query(from_, to_, arrival_time, week_day, min_prob):\n",
    "    \n",
    "    df = timetables[week_day.lower()]\n",
    "    \n",
    "    avoid=[]\n",
    "    alternatives = []\n",
    "    probabilities = []\n",
    "    i = 0\n",
    "    prob = 0\n",
    "    \n",
    "    # Check if stop is in dataset\n",
    "    if ((from_ not in df[\"departure_stop\"].values) | (to_ not in df[\"arrival_stop\"].values)) :\n",
    "        return [], [\"missing\"]\n",
    "    \n",
    "    for i in range(3):\n",
    "    \n",
    "        path = CSA(from_, to_, arrival_time, df, avoid)\n",
    "        if (len(path) > 0) :\n",
    "            for c in reversed(path):\n",
    "                if c[\"trip_id\"] != \"Walking\":\n",
    "                    avoid.append(c[\"trip_id\"])\n",
    "                    break\n",
    "            \n",
    "            prob = confidence_interval(path, week_day, arrival_time)\n",
    "            path = fix_walking(path)\n",
    "\n",
    "            if prob >= min_prob:\n",
    "                alternatives.append(path)\n",
    "                probabilities.append(prob)\n",
    "\n",
    "            if prob == 1:\n",
    "                break\n",
    "    \n",
    "    return alternatives, probabilities\n",
    "\n",
    "\n",
    "import json\n",
    "from pyspark.sql.types import StringType\n",
    "def queryFromLocalContext(from_, to_, arrival_time, week_day, min_prob) : \n",
    "    \"\"\"\n",
    "    In order to send the data to the local context, we need to embedded the output in a Spark DataFrame, which can be serialized and sent through the network. \n",
    "    In particular, we use the python library`json` to convert the output of the algorithm (a list of journeys) into a string which can be embedded in a Spark DataFrame. \n",
    "    \"\"\"\n",
    "    journeys, probabilities = query(from_, to_, arrival_time, week_day, min_prob)\n",
    "    return spark.createDataFrame([json.dumps([[{k: str(v) for k, v in x.to_dict().items()} for x in list_] for list_ in journeys]), json.dumps(probabilities)], StringType())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ef879cb-43cd-4bc0-8d00-15e89cc96ba1",
   "metadata": {},
   "source": [
    "## 4. Data loading"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "550011cd-1ef8-4fe0-99d4-9edc6321b91b",
   "metadata": {},
   "source": [
    "In this section we load all the precomputed data of section 1. \n",
    "Note that the Ricola Journey planner can be ran by simply executing sections 2 to 6, skipping section 1, provided that the precomputed data is already stored on the cluster. \n",
    "In particular, the filtered `istdaten` dataset, used to compute the probabilities is loaded using PySpark, while the other datasets are loaded into the memory of the master node, as pandas dataframes. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd1e65c4-f8a6-48d0-bfbc-fd9ffcef7062",
   "metadata": {},
   "source": [
    "##### Timetable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b3303b0-88da-4e05-98ef-276c0adc4cad",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%spark\n",
    "from pyspark.sql import functions as F\n",
    "def getDatasetOfDay(week_day) : \n",
    "    df = spark.read.option(\"header\",True).option(\"encoding\", \"UTF-8\").csv(\"/group/ricola/\"+week_day.lower()+\".csv\").toPandas()\n",
    "    df['arrival_time'] = pd.to_datetime(df['arrival_time'], format= '%H:%M:%S' ).dt.time\n",
    "    df['departure_time'] = pd.to_datetime(df['departure_time'], format= '%H:%M:%S' ).dt.time\n",
    "    \n",
    "    for column in [\"trip_id\", \"departure_stop\", \"arrival_stop\"] : \n",
    "        df[column] = df[column].apply(lambda x: x.encode(\"utf-8\"))\n",
    "\n",
    "    df = df.sort_values(by=['arrival_time'], ascending=False)\n",
    "    return df\n",
    "\n",
    "timetables = {}\n",
    "for day in ['monday', 'tuesday', 'wednesday'] : \n",
    "    timetables[day] = getDatasetOfDay(day)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25ea2e78-1782-4cb1-84f9-d50ee5556949",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%spark\n",
    "for day in ['thursday', 'friday'] : \n",
    "    timetables[day] = getDatasetOfDay(day)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d03c3ed-273d-4280-b614-e6a27d9a9275",
   "metadata": {},
   "source": [
    "##### Zurich delays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4737181-3d25-422b-9083-02ece685870c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%spark\n",
    "zurich_istdaten_delays = spark.read.option(\"header\",True).option(\"encoding\", \"UTF-8\").orc(\"/group/ricola/zurich_istdaten_delays.orc\").cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7d805a9-48cf-4f5d-af28-afdcd220a644",
   "metadata": {},
   "source": [
    "##### Walking paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7691f121-b615-4818-8933-3689c65b282b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%spark\n",
    "short_walks = spark.read.option(\"header\",True).option(\"encoding\", \"UTF-8\").csv(\"/group/ricola/short_walks.csv\").toPandas()\n",
    "short_walks[\"duration\"] = short_walks[\"duration\"].apply(lambda x: pd.to_timedelta(int(x), unit='m'))\n",
    "\n",
    "for column in [\"departure\", \"arrival\"] : \n",
    "    short_walks[column] = short_walks[column].apply(lambda x: x.encode(\"utf-8\"))\n",
    "footpaths = short_walks\n",
    "journey_planning_output = spark.createDataFrame([], StringType())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4663903-5f82-446c-8490-d5f733d29c76",
   "metadata": {},
   "source": [
    "##### Zurich stops\n",
    "We need to move locally the dataset containing all the stops in Zurich. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3daf56b4-fa20-493d-982f-0d986dacf9f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_filepath = os.path.join('../data')\n",
    "if 'zurich_stops' not in os.listdir(data_filepath):\n",
    "    !/usr/hdp/current/hadoop-3.1.1/bin/hdfs dfs -copyToLocal /group/ricola/zurich_stops.csv ../data/zurich_stops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67d984a9-4742-4646-96aa-f4c8a494d7ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import glob\n",
    "import pandas as pd\n",
    "csv_file = glob.glob(\"../data/zurich_stops/*.csv\")[0]\n",
    "zurich_stops = pd.read_csv(csv_file)\n",
    "zurich_stops = zurich_stops.drop(columns=[\"stop_id\", \"location_type\", \"parent_station\", \"distance_from_zurich\"]).sort_values(by=[\"stop_name\"])\n",
    "#zurich_stops[\"stop_name\"] = zurich_stops[\"stop_name\"].apply(lambda x: x.encode(\"utf-8\"))\n",
    "# We convert it to a dictionary with key `stop_name` and value latitute and longitude\n",
    "zurich_stops = dict(zip(zurich_stops[\"stop_name\"], zip(zurich_stops[\"stop_lat\"], zurich_stops[\"stop_lon\"])))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91a8f35e-8db6-4956-9b70-f36d22f355bb",
   "metadata": {},
   "source": [
    "# 5. Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f78875a5-2a97-410e-a6a9-94da4fd5bb59",
   "metadata": {},
   "source": [
    "#### Validation CSA algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56db9dbe-6de0-415b-b896-190e3e85f98d",
   "metadata": {},
   "source": [
    "We first validate the output of the CSA algorithm by comparing the suggested trips to those indicated on SBB and Google Maps. The trips we will try are the following:\n",
    "- From \"Zrich Oerlikon\" to \"Zrich Wollishofen\" on a Wednesday with arrival within 11:20am\n",
    "- From \"Ringlikon\" to \"Rmlang\" on a Monday with arrival within 7:20pm (This trip is within the city of Zurich and thus shoudl test Trams and Buses)\n",
    "- From \"Dbendorf\" to \"Zrich Altstetten\" on a Tuesday with arrival within 3pm \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27b488ab-a9a7-4d3d-834c-c161e57590c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%spark\n",
    "result = CSA('Zrich Oerlikon', 'Zrich Wollishofen', time(11, 20, 0), timetables['wednesday'], avoid = [])\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc84e48a-5d96-4409-aee1-8cf69a8139c4",
   "metadata": {},
   "source": [
    "We notice that the selected journey corresponds to what is suggested on the SBB app. This is a relatively easy journey as the two selected stations are directly connected by public transports, however it works well as a simple proof of concept. \n",
    "\n",
    "In the next cell we test two stations that are not directly connected and thus require to change the mean of transportation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ab14385-77ae-4cf6-a598-59b0cee5840f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%spark\n",
    "result = CSA('Dbendorf', 'Zrich Altstetten', time(14, 55, 0), timetables['monday'], avoid = [])\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a9c8a5b-6b1a-4120-9767-f7715b7778a5",
   "metadata": {},
   "source": [
    "Once again, the outcome of our implementation of the CSA algorithm corresponds to the journey suggested on the SBB app.\n",
    "\n",
    "As a final test we experiment with two locations that are hardly reacheable with public transport and involve multiple changes. As can be seen if tried on Google Maps, the fact that many changes are need adds many dergees of freedom to the algorithm and thus the outcome is likely to be different compared to that suggested on the SBB app."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4de1bbe8-6f4d-44c6-810e-a4fc082692aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%spark\n",
    "result = CSA('Wildpark-Hfli', 'Ksnacht Goldbach', time(19, 20, 0), timetables['tuesday'], avoid = [])\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a47d659-e08f-4504-98ea-275dd4dce0b9",
   "metadata": {},
   "source": [
    "We notice that the solution of our algorithm corresponds to that from the SBB app. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b019a630-a0ac-4165-92fc-d12c8042b233",
   "metadata": {},
   "source": [
    "#### Validation Robust Journey Planner"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7713e549-2d9c-4673-859f-e216f17d1e11",
   "metadata": {},
   "source": [
    "Using the same journeys as above, we validate now the overall architecture of our Journey Planner, including the computation of the probability of success of a trip. For now, we set the minimum probability to be 0.5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a763524-6f39-4c75-be07-96d3ba65bde9",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%spark\n",
    "result = query('Zrich Oerlikon', 'Zrich Wollishofen', time(11, 20, 0), 'Wednesday', 0.5)\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6dc6a47-876f-4bde-b660-8c9f91dd6d86",
   "metadata": {},
   "source": [
    "We notice that the Planner returns three trips and theirs probabilities of success are estimated to be relatively high. \n",
    "\n",
    "The first suggestion is the direct train and its high proabbility of success is reasonable as the arrival time is 6 minutes before the required arrival time. The second trip includes a change at the Zuirch station with 3 miuntes to change. We acknowledge that, given the dimensions of Zurich HB, this might be an overestimation of the probability of success. For the next version of the planner, we could include information about the platform of arrival-departure to improve the precision of the probability estimation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff865731-0b05-47c7-83f1-22d07307e86d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%spark\n",
    "result = query('Dbendorf', 'Zrich Altstetten', time(14, 55, 0), 'Monday', 0.5)\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e7337f2-f63b-440d-a2c2-0c6a98aae9b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%spark\n",
    "result = query('Wildpark-Hfli', 'Ksnacht Goldbach', time(19, 20, 0), 'Tuesday', 0.5)\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87e61ecf-c949-4cfc-aaf5-817519e95a18",
   "metadata": {},
   "source": [
    "# 6. User interface"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75866801-93dc-4333-8e96-e543204d1cbd",
   "metadata": {},
   "source": [
    "Our solution involves computation of journeys on the cluster. Although we possess limited computational resources in this project, we thought that such a project in a real-world situation would involve the computation of journeys and related probabilities in a distributed system, consequently we looked for a way to enable I/O communication of our GUI with the cluster. \n",
    "\n",
    "##### Connection with Spark\n",
    "The GUI we provided communicates with the cluster sending HTTP requests and uses the jupyter magic `%spark` to retrieve the output. In order to make it work, we need to find out the Spark session id. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f780d6c6-29e7-4cc9-9610-c7da03ffca83",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%spark -o application_id\n",
    "from pyspark.sql.types import StringType\n",
    "application_id = spark.sparkContext.applicationId\n",
    "application_id = spark.createDataFrame([application_id], StringType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53a3ea35-7d7b-4ca7-a890-5a1c124e24e5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json, requests\n",
    "def find_session_id(sessions_json, application_id) : \n",
    "    for session in sessions_json : \n",
    "        if session[\"appId\"] == application_id : \n",
    "            return session['id']\n",
    "    return None\n",
    "\n",
    "host = server\n",
    "headers = {'Content-Type': 'application/json'}\n",
    "sessions_url = f\"{host}/sessions\"\n",
    "sessions_request = requests.get(sessions_url, headers=headers)\n",
    "session_id = find_session_id(sessions_request.json().get('sessions'), application_id[\"value\"][0])\n",
    "print(\"Your session id is\", session_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95a17ee7-f868-4c3f-a5ad-70dbedb9e2bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "def send_request_to_spark(from_, to, arrival_hour, arrival_minute, weekday, min_prob) : \n",
    "    \"\"\"\n",
    "    This function forwards a request to the cluster. We use an HTTP request to execute some code in the cluster (we execute the code contained in the variable `pyspark_code`). \n",
    "    The global variable `session_id`, which corresponds to the Spark session id on the cluster, has to be defined prior to running this function. \n",
    "    \n",
    "    Returns a list of pandas DataFrames\n",
    "    \"\"\"\n",
    "    global journey_planning_output\n",
    "    \n",
    "    # Configure remote host\n",
    "    host = server\n",
    "    headers = {'Content-Type': 'application/json'}\n",
    "    sessions_url = f\"{host}/sessions\"\n",
    "    statements_url = f\"{sessions_url}/{session_id}/statements\"\n",
    "    \n",
    "    # Set function call on Spark and output variable\n",
    "    function = \"\"\"queryFromLocalContext(\"{0}\", \"{1}\", time({2}, {3}, 0), \"{4}\", {5})\"\"\".format(from_, to, arrival_hour, arrival_minute, weekday, min_prob)\n",
    "    var_name = \"journey_planning_output\"\n",
    "\n",
    "    # Build pyspark code\n",
    "    pyspark_code = u\"\"\"del {0}; {0} = {1}\"\"\".format(var_name, function)\n",
    "\n",
    "    # Send request\n",
    "    b1 = requests.post(statements_url, data=json.dumps({'code': pyspark_code}), headers=headers)\n",
    "    b1.json()\n",
    "    \n",
    "    journey_planning_output = \"\"\n",
    "    while (len(journey_planning_output) == 0) : \n",
    "        time.sleep(1)\n",
    "        %spark -o journey_planning_output -n -1\n",
    "    \n",
    "    output_journeys = json.loads(journey_planning_output[\"value\"][0])\n",
    "    output_probabilities = json.loads(journey_planning_output[\"value\"][1])\n",
    "\n",
    "    # Construct a list of pandas DataFrames from the output\n",
    "    return [pd.DataFrame([pd.Series(trip) for trip in journey]) for journey in output_journeys], output_probabilities"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b07a61fc-9241-405b-9a56-41d4f9606644",
   "metadata": {},
   "source": [
    "bubu = \"asd\";"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "636d942f-d266-4b9e-90fa-456d5635a9fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "days_of_week = [\"Monday\", \"Tuesday\", \"Wednesday\", \"Thursday\", \"Friday\"]\n",
    "\n",
    "example_journey = pd.DataFrame([\n",
    "    [\"Train IC152\", 47.37809, 8.54031, 47.38523, 8.54254, \"Zurich HB\", \"Ottikerstrasse\"], \n",
    "    [\"Bus 421\", 47.38523, 8.54254, 47.37777, 8.54829, \"Ottikerstrasse\", \"ETH/Universitatsspital\"], \n",
    "    [\"Foot 500m\", 47.37777, 8.54829, 47.36718, 8.54513, \"ETH/Universitatsspital\", \"Bellevue\"]\n",
    "], columns=[\"means_of_transport\", \"start_latitude\", \"start_longitude\", \"arrival_latitude\", \"arrival_longiture\", \"start_station\", \"arrival_station\"])\n",
    "example_journeys = [example_journey, example_journey, example_journey]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "079e91f2-f97a-42d3-8130-f95e7c7bb046",
   "metadata": {},
   "source": [
    "##### Graphical user interface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be434d40-a2f9-44ee-a47e-6fb4779c4afe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "MAPBOX_TOKEN_GORSI = \"pk.eyJ1IjoiZ2lhY29tb29yc2kiLCJhIjoiY2pubTM0Nml6MW02MDNwcWY0ajc3ZHE3diJ9.fz0p1ZmseERTYVzXJPqS0Q\"\n",
    "GEOPS_API_KEY = \"5cc87b12d7c5370001c1d655ec23ec0cdd5242b08c54811292516f60\"\n",
    "px.set_mapbox_access_token(MAPBOX_TOKEN_GORSI)\n",
    "\n",
    "from ipywidgets import Accordion\n",
    "\n",
    "# Results page generator \n",
    "def show_results(journeys, probabilities) : \n",
    "    \n",
    "    if (len(journeys) == 0) : \n",
    "            return HTML(\"<h3>We are sorry, we couldn't find any journey with the given parameters. \")\n",
    "    \n",
    "    def generate_summary_journey(journey, i) : \n",
    "        last_trip = journey.shape[0]-1\n",
    "        out = \"{0} ({1}) -> {2} ({3})   ({4} changes, probability {5})\".format(journey[\"departure_stop\"][0], journey[\"departure_time\"][0], journey[\"arrival_stop\"][last_trip], journey[\"arrival_time\"][last_trip], journey.shape[0]-1, round(probabilities[i], 2))\n",
    "        \n",
    "        return out\n",
    "    \n",
    "    def generate_summary_trip(trip) : \n",
    "        return \"{0} {1} from {2} ({3}) to {4} ({5})\".format(trip[\"transportation_type\"], trip[\"transportation_name\"], trip[\"departure_stop\"], trip[\"departure_time\"], trip[\"arrival_stop\"], trip[\"arrival_time\"])\n",
    "        \n",
    "    \n",
    "    def generate_maps(journeys) : \n",
    "        \n",
    "        line_colors = {'Zug':'red', 'Walking':'grey', 'Tram': 'brown', 'Bus': 'blue'}\n",
    "        \n",
    "        maps = []\n",
    "        for journey in journeys : \n",
    "            fig = go.FigureWidget(layout={\"autosize\":False, \"width\":1000})\n",
    "            for (i, row) in journey.iterrows() : \n",
    "                fig.add_trace(go.Scattermapbox(\n",
    "                    mode = \"markers+lines\",\n",
    "                    lat = [zurich_stops[row[\"departure_stop\"]][0], zurich_stops[row[\"arrival_stop\"]][0]],\n",
    "                    lon = [zurich_stops[row[\"departure_stop\"]][1], zurich_stops[row[\"arrival_stop\"]][1]],\n",
    "                    marker = {'size': 15},\n",
    "                    line = go.scattermapbox.Line(width=(2 if row[\"transportation_type\"] == \"Walking\" else 5), color=line_colors.get(row[\"transportation_type\"], 'black')),\n",
    "                    text = generate_summary_trip(row),\n",
    "                    name = \"{} {}\".format(row[\"transportation_type\"], row[\"transportation_name\"])  \n",
    "                )\n",
    "                )\n",
    "            fig.update_geos(\n",
    "                projection_type=\"orthographic\"\n",
    "            )\n",
    "            fig.update_layout(\n",
    "                autosize=False,\n",
    "                margin ={'l':0,'t':0,'b':0,'r':0},\n",
    "                width=1000,\n",
    "                mapbox = dict(\n",
    "                    style=\"https://maps.geops.io/styles/base_bright_v2/style.json?key=\"+GEOPS_API_KEY,\n",
    "                    accesstoken=MAPBOX_TOKEN_GORSI,\n",
    "                    center=dict(lat=47.37809, lon=8.54031),\n",
    "                    zoom=11\n",
    "                ),\n",
    "            )\n",
    "            fig.update_layout(\n",
    "                width=1000    \n",
    "            )\n",
    "\n",
    "            maps += [fig]\n",
    "        return maps\n",
    "    \n",
    "    def on_click_accordion(e) :\n",
    "        # Update widgets every time a user clicks on the accordion in order to reload the map\n",
    "        output.clear_output(wait=True)\n",
    "        with output:\n",
    "            display(VBox([form, results_view]))\n",
    "    \n",
    "    maps = generate_maps(journeys)\n",
    "    solutions = []\n",
    "    for (i, journey) in enumerate(journeys) : \n",
    "        elements = []\n",
    "        for (j, trip) in journey.iterrows() : \n",
    "            \n",
    "            elements += [HBox([Label(generate_summary_trip(trip))])]\n",
    "\n",
    "        elements += [maps[i]]#[[go.FigureWidget(maps[i].to_dict())]\n",
    "        solutions += [VBox(elements)]\n",
    "\n",
    "    accordion = widgets.Accordion(children=solutions, selected_index=None)\n",
    "\n",
    "    for (i, journey) in enumerate(journeys) : \n",
    "        accordion.set_title(i, generate_summary_journey(journey, i))\n",
    "\n",
    "\n",
    "    footer = HTML(\"<h3>Bubuxino</h3>\")\n",
    "    results_page = VBox([HTML(\"<h1>Results</h1>\"), accordion, footer])\n",
    "    \n",
    "    accordion.observe(on_click_accordion, names=\"selected_index\")\n",
    "        \n",
    "    return accordion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b660b045-42e9-4dec-99bd-eb141c0982b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ipywidgets import Layout, Button, Textarea, Dropdown, Label, IntSlider, Combobox, BoundedIntText, HTML, HBox, GridBox, VBox\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display, clear_output\n",
    "\n",
    "\n",
    "### Graphical interface view controller\n",
    "\n",
    "\n",
    "departure_station_dropdown = Dropdown(options=list(zurich_stops.keys()), ensure_option=True, disabled=False)\n",
    "arrival_station_dropdown   = Dropdown(options=list(zurich_stops.keys()), ensure_option=True, disabled=False)\n",
    "confidence_slider          = IntSlider(min=0, max=100, value=80)\n",
    "day_of_week_dropdown       = Dropdown(options=days_of_week)\n",
    "hour_text                  = BoundedIntText(value=12, min=0, max=23, disabled=False, layout=Layout(width=\"50px\"))\n",
    "minute_text                = BoundedIntText(value=30, min=0, max=59, disabled=False,  layout=Layout(width=\"50px\"))\n",
    "search_button              = Button(description='Search journey', disabled=False, button_style='info', tooltip='Click here to search for a journey', icon='search')\n",
    "\n",
    "loader                     = HTML(\"\"\"<style>.loader{border:5px solid #f3f3f3;border-radius:50%;border-top:5px solid #3498db;width:20px;height:20px;-webkit-animation:spin 2s linear infinite;animation:spin 2s linear infinite}@-webkit-keyframes spin{0%{-webkit-transform:rotate(0deg)}100%{-webkit-transform:rotate(360deg)}}@keyframes spin{0%{transform:rotate(0deg)}100%{transform:rotate(360deg)}}</style>\n",
    "<div style=\"margin-top:20px\">Request received. Please wait... <div class=\"loader\" style=\"display:inline-block;vertical-align:middle\"></div></div>\"\"\")\n",
    "error_validation_view      = HTML(\"\"\"<h4 style=\"margin-top: 20px\">Please check the data you have inserted</h4>\"\"\")\n",
    "\n",
    "form_items = [\n",
    "    Label(value='Departure Station'), departure_station_dropdown,\n",
    "    Label(value='Arrival Station'), arrival_station_dropdown,\n",
    "    Label(value='Day of the week'), day_of_week_dropdown,\n",
    "    Label(value='Time of arrival'), HBox([hour_text,Label(value=\":\"), minute_text]),\n",
    "    Label(value='Minimum confidence'), confidence_slider\n",
    "]\n",
    "\n",
    "grid = GridBox(form_items, layout=Layout(grid_template_columns=\"repeat(2, 300px)\"))\n",
    "form = VBox([HTML(\"<h1>Ricola Journey Planner</h1>\"), grid, search_button], layout=Layout(padding=\"20px\", border=\"solid 2px\", width=\"650px\"))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2406655d-bebb-4574-80cb-61822f8b9a2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = widgets.Output()\n",
    "results = None\n",
    "\n",
    "def on_search(b):\n",
    "    form_data = {\"departure\":departure_station_dropdown.value, \"arrival\":arrival_station_dropdown.value, \"day\": day_of_week_dropdown.value, \"hour\": hour_text.value, \"minute\": minute_text.value, \"confidence\" : float(confidence_slider.value)/100, \"weekday\": day_of_week_dropdown.value }\n",
    "    if (check_gui_query(form_data)) : \n",
    "        execute_query(form_data)\n",
    "    else :\n",
    "        with output: \n",
    "            output.clear_output(wait=True)\n",
    "            display(VBox([form, error_validation_view]))\n",
    "            \n",
    "def check_gui_query(q) : \n",
    "    stations = list(zurich_stops.keys())\n",
    "    bubu =  (q[\"departure\"] in stations) and (q[\"arrival\"] in stations) and (q[\"day\"] != \"\") and (q[\"hour\"] != \"\") and (q[\"minute\"] != \"\") and (q[\"confidence\"] != \"\")\n",
    "    return bubu\n",
    "\n",
    "\n",
    "def execute_query(data) : \n",
    "    global results_view \n",
    "    output.clear_output(wait=True)\n",
    "    with output: \n",
    "        display(VBox([form, loader]))\n",
    "\n",
    "        journeys, probabilities = send_request_to_spark(data[\"departure\"], data[\"arrival\"], data[\"hour\"], data[\"minute\"], data[\"weekday\"], data[\"confidence\"])\n",
    "        #print(journeys)\n",
    "\n",
    "        results_view = show_results(journeys, probabilities)\n",
    "        output.clear_output(wait=True)\n",
    "\n",
    "        display(VBox([form, results_view]))\n",
    "\n",
    "search_button.on_click(on_search)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d2fecdb-0ae6-4e7b-bafd-372959a689e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the interface\n",
    "with output : \n",
    "    display(form)\n",
    "    \n",
    "output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39b0fb7b-8687-408b-b16e-47c406fc980b",
   "metadata": {},
   "source": [
    "**PS** Do not get scared of \"Current status is busy\", a suggested trip is underway!\n",
    "\n",
    "#### Aknowledgements\n",
    "The map shown above is the official SBB map that can be found on the SBB website and apps. We contacted geOps GmbH, that is the company that manages the map platform for SBB and they were so kind to provide us a free API key to use their map service."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
